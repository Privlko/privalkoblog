---
title: Visualising LME output
author: Ivan
date: '2018-09-25'
slug: visualising-lme-output
categories:
  - dataviz
  - R
  - Rstats
tags: []
---



<p>In this post I want to summarise a quick method for summarising random-effects output.</p>
<p>I’ve been working with multi-level models recently. During grad school I spent a lot of time using fixed-effects methods, which are less prone to bias but <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S2049847014000077">drop lots of information between cluster groups</a>. Random effects models also have issues, but new methods have made it easier to overcome these.</p>
<p>There are tonnes of interesting courses and books on random effects modelling, which are worth reading. One point that’s rarely mentioned is how to visualise the output from a random effects model. This is particularly annoying because the estimates don’t contain p-values, and there are issues with using <code>broom()</code> (click <a href="https://github.com/tidymodels/broom/issues/96">here</a> for a thread).</p>
<pre class="r"><code>summary(m1)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: wage ~ gender + mob + (1 | id)
##    Data: q2
## 
## REML criterion at convergence: 1081526
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -14.6955  -0.3293  -0.0852   0.2477  29.2790 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  id       (Intercept) 175796690 13259   
##  Residual              89596345  9466   
## Number of obs: 49739, groups:  id, 16748
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  22942.9      167.7 136.839
## genderF      -7364.8      229.0 -32.161
## mobPromo      1783.4      245.4   7.269
## mobQuit        111.2      208.1   0.534
## 
## Correlation of Fixed Effects:
##          (Intr) gendrF mobPrm
## genderF  -0.719              
## mobPromo -0.088  0.005       
## mobQuit  -0.133  0.029  0.074</code></pre>
<p>The output above has no p-values. You could use the standard errors to gage what’s significant, but generally it’s hard to unpick what’s important and what’s not. If this was an LM function, we could use <code>broom()</code> but <code>broom()</code> is a bit complex when the data is hierarchical. Instead we can use <code>fixef()</code> and <code>confint()</code> to save some estimates in a data frame, before plotting.</p>
<pre class="r"><code>dataPlot &lt;- data.frame(cbind( fixef(m1), confint(m1)[ 3:6, ]))
rownames(dataPlot)[1] &lt;- &quot;Intercept&quot;
colnames(dataPlot) &lt;- c(&quot;mean&quot;, &quot;l95&quot;, &quot;u95&quot;)
dataPlot$parameter &lt;- rownames(dataPlot)

dataPlot</code></pre>
<pre><code>##                 mean        l95        u95 parameter
## Intercept 22942.9399 22614.3341 23271.5647 Intercept
## genderF   -7364.7969 -7813.6177 -6915.9817   genderF
## mobPromo   1783.3951  1301.2284  2266.0280  mobPromo
## mobQuit     111.2188  -296.7092   519.2626   mobQuit</code></pre>
<p>Using the newly created <code>dataPlot</code>, we can send the coefficients to <code>ggplot()</code> and visualise these.</p>
<p><img src="/post/2018-09-25-visualising-lme-output_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The resulting plot makes it easier to point out significant estimates. For example above, the chart above suggests that quitting has no impact on wages, but promotions do. Gender has a negative impact on pay and the pay for men who stay in the same job for a given year is roughly 25,000 rubles.</p>
<p>I’m looking forward to using these plots further. For now, they are a great way to illustrate the data and single out important estimates.</p>
